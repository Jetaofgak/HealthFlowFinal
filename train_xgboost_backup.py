# %%
# CELL 1: Imports
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import time
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %%
# CELL 2: Configuration
CSV_PATH = r"dataset_with_nlp.csv"  # Fixed filename - auto-generated by extract_biobert_features.py
MODEL_FILENAME = "xgboost_readmission_model.ubj"
OPTIONAL_JSON_EXPORT = os.getenv("EXPORT_JSON_MODEL", "false").lower() == "true"
LEGACY_JSON_PATH = "xgboost_readmission_model.json"
TARGET_COL = 'label_readmission'
NULL_THRESHOLD = 0.80  # Drop columns with >80% null

print(f"ğŸ“‹ Configuration: {TARGET_COL} | Null threshold: {NULL_THRESHOLD*100}%")


# %%
# CELL 3: Load Raw Data & Preprocess (Using Shared Loader)
import time
from utils.data_loader import load_and_preprocess_data, get_train_val_test_splits

print(f"ğŸ“‚ Chargement de {CSV_PATH} ...")
start_time = time.time()

# XGBoost handles NaNs natively, so we don't impute
df, _ = load_and_preprocess_data(CSV_PATH, TARGET_COL, impute_missing=False)
X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_splits(df, TARGET_COL)

print(f"âœ… Dataset chargÃ© et divisÃ© en {time.time() - start_time:.2f}s")
print(f"âœ… Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}")
print(f"ğŸ“Š Train distribution:\n{y_train.value_counts(normalize=True).round(3)}")
print(f"ğŸ“Š Val distribution:\n{y_val.value_counts(normalize=True).round(3)}")



# %%
# CELL 13: Device Detection & Model Configuration
def detect_device() -> str:
    if torch.cuda.is_available():
        print("ğŸŸ¢ CUDA dÃ©tectÃ© : utilisation du GPU NVIDIA.")
        return "cuda"
    print("â„¹ï¸ CPU uniquement (pas de support CUDA/MPS pour XGBoost).")
    return "cpu"

device = detect_device()
print(f"ğŸš€ Initialisation de XGBoost sur {device.upper()}...")

model = xgb.XGBClassifier(
    device=device,
    tree_method="hist",
    n_estimators=667,
    learning_rate=0.08136324223569548,
    max_depth=5,
    subsample=0.689209825278097,
    colsample_bytree=0.72327888220896,
    objective='binary:logistic',
    eval_metric='auc',
    early_stopping_rounds=50,
    missing=float('nan'),
    reg_alpha=1.7749145784633522,
    reg_lambda=6.56042094848319,
    gamma=1.2238697724183818,
    min_child_weight=9
)

print("âœ… ModÃ¨le configurÃ©!")


# %%
# CELL 13: Training WITH Train/Val/Test Tracking
print("ğŸ”¥ === ENTRAÃNEMENT Train/Val/Test tracking ===\n")
start_train = time.time()

model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_val, y_val)],  # Use validation set for early stopping
    verbose=100
)

print(f"âœ… TerminÃ© en {time.time() - start_time:.2f}s")
print(f"ğŸ† Best iteration: {model.best_iteration}")


# %%
# CELL 14.5: Extract Training History (StackOverflow #1 method)
import matplotlib.pyplot as plt
import numpy as np

# Get evaluation results directly from XGBoost
results = model.evals_result()
epochs = len(results['validation_0']['auc'])
x_axis = range(0, epochs)

print(f"ğŸ“Š {epochs} iterations tracked")
print(f"ğŸ† Best iteration: {model.best_iteration}")
print(f"ğŸ“ˆ Peak AUC: {max(results['validation_0']['auc']):.4f}")


# %%
# CELL 15: Train vs Test Dashboard (FINAL FIXED VERSION)
results = model.evals_result()

# Auto-detect keys
train_key = 'validation_0'  # Train set
test_key = 'validation_1'   # Test set

epochs = len(results[test_key]['auc'])
x_axis = range(0, epochs)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('XGBoost: Train vs Test Curves', fontsize=16, fontweight='bold')

# 1. AUC Curves
axes[0,0].plot(x_axis, results[train_key]['auc'], 'g-', linewidth=3, label='Train AUC')
axes[0,0].plot(x_axis, results[test_key]['auc'], 'b-', linewidth=3, label='Test AUC')  
axes[0,0].axvline(model.best_iteration, color='red', linestyle='--', linewidth=2)
axes[0,0].set_title('AUC: Train vs Test')
axes[0,0].legend(); axes[0,0].grid(True, alpha=0.3)

# 2. Loss Curves
train_loss = 1 - np.array(results[train_key]['auc'])
test_loss = 1 - np.array(results[test_key]['auc'])
axes[0,1].plot(x_axis, train_loss, 'g-', linewidth=3, label='Train Loss')
axes[0,1].plot(x_axis, test_loss, 'r-', linewidth=3, label='Test Loss')
axes[0,1].axvline(model.best_iteration, color='red', linestyle='--')
axes[0,1].set_title('Loss: Train vs Test')
axes[0,1].legend(); axes[0,1].grid(True, alpha=0.3)

# 3. Overfitting Gap
gap = np.array(results[train_key]['auc']) - np.array(results[test_key]['auc'])
axes[1,0].plot(x_axis, gap, 'purple', linewidth=3)
axes[1,0].axhline(0, color='black', linestyle='-', alpha=0.5)
axes[1,0].axvline(model.best_iteration, color='red', linestyle='--')
axes[1,0].set_title('Overfitting Gap (Train-Test AUC)')
axes[1,0].set_ylabel('AUC Gap'); axes[1,0].grid(True, alpha=0.3)

# 4. Feature Importance
feature_imp = pd.DataFrame({'feature': X_train.columns, 'importance': model.feature_importances_}).sort_values('importance', ascending=False).head(10)
sns.barplot(data=feature_imp, y='feature', x='importance', ax=axes[1,1])
axes[1,1].set_title('Top 10 Features')

plt.tight_layout()
plt.show()

print(f"Train AUC: {results[train_key]['auc'][-1]:.4f}")
print(f"Test AUC:  {results[test_key]['auc'][-1]:.4f}")
print(f"Max Gap:   {max(gap):.4f}")


# %%
# CELL 15: Generate Predictions
print("ğŸ”® === GÃ‰NÃ‰RATION DES PRÃ‰DICTIONS ===\n")

# Binary predictions (0 or 1)
y_pred = model.predict(X_test)

# Probability predictions for ROC curve
y_prob = model.predict_proba(X_test)[:, 1]

print(f"âœ… Predictions generated: {len(y_pred)} samples")
print(f"ğŸ“Š Prediction distribution:\n{pd.Series(y_pred).value_counts()}")

# Calculate metrics
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

print(f"\nğŸ¯ Test Accuracy: {accuracy:.4f}")
print(f"ğŸ“ˆ Test ROC-AUC: {roc_auc:.4f}")
print(f"\nğŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred, target_names=['No Readmission', 'Readmission']))

# Generate and visualize confusion matrix
print(f"\nğŸ”² === CONFUSION MATRIX ===\n")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Visualize confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(
    cm, 
    annot=True, 
    fmt='d', 
    cmap='Blues',
    xticklabels=['No Readmission', 'Readmission'],
    yticklabels=['No Readmission', 'Readmission'],
    cbar_kws={'label': 'Count'}
)
plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')
plt.ylabel('Actual', fontsize=12)
plt.xlabel('Predicted', fontsize=12)
plt.tight_layout()
plt.show()

# Calculate and display confusion matrix metrics
tn, fp, fn, tp = cm.ravel()
print(f"\nğŸ“Š Confusion Matrix Breakdown:")
print(f"   True Negatives (TN):  {tn:>6} - Correctly predicted No Readmission")
print(f"   False Positives (FP): {fp:>6} - Incorrectly predicted Readmission")
print(f"   False Negatives (FN): {fn:>6} - Missed Readmissions")
print(f"   True Positives (TP):  {tp:>6} - Correctly predicted Readmission")

# Calculate additional metrics
sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
f1_score = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0

print(f"\nğŸ“ˆ Additional Metrics:")
print(f"   Sensitivity (Recall): {sensitivity:.4f} - % of actual readmissions caught")
print(f"   Specificity:          {specificity:.4f} - % of non-readmissions correctly identified")
print(f"   Precision:            {precision:.4f} - % of predicted readmissions that are correct")
print(f"   F1-Score:             {f1_score:.4f} - Harmonic mean of precision and recall")


# %%
# CELL 16: Save Model
booster = model.get_booster()
booster.save_model(MODEL_FILENAME)
print(f"ğŸ’¾ ModÃ¨le sauvegardÃ©: {MODEL_FILENAME}")

if OPTIONAL_JSON_EXPORT:
    booster.save_model(LEGACY_JSON_PATH)
    print(f"ğŸ“ JSON export: {LEGACY_JSON_PATH}")

print("ğŸ‰ Pipeline complet terminÃ©!")



